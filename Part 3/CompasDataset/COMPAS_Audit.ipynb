

# 📌 COMPAS Recidivism Dataset Bias Audit


# -------------------------------
# 📚 1. Install and Import Libraries
# -------------------------------

# Install AI Fairness 360 (if not already installed)
# !pip install aif360

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from aif360.datasets import CompasDataset
from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric
from aif360.algorithms.preprocessing import Reweighing
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

# -------------------------------
# 📖 2. Load COMPAS Dataset
# -------------------------------

dataset = CompasDataset()

# View dataset shape and feature names
print("Dataset shape:", dataset.features.shape)
print("Features:", dataset.feature_names)
print("Protected attribute:", dataset.protected_attribute_names)

# -------------------------------
# 📊 3. Define Protected Attribute & Label
# -------------------------------

privileged_groups = [{'race': 1}]  # 1 = Caucasian
unprivileged_groups = [{'race': 0}]  # 0 = African-American

# -------------------------------
# 📏 4. Compute Fairness Metrics (Original Dataset)
# -------------------------------

metric_orig = BinaryLabelDatasetMetric(dataset,
                                       unprivileged_groups=unprivileged_groups,
                                       privileged_groups=privileged_groups)

print("Disparate Impact (original):", metric_orig.disparate_impact())
print("Mean difference:", metric_orig.mean_difference())

# -------------------------------
# 📊 5. Visualize False Positive Rate by Race
# -------------------------------

# Prepare DataFrame for plotting
df = pd.DataFrame(dataset.convert_to_dataframe()[0])
df['prediction'] = dataset.labels.ravel()

fpr_african_american = df[(df['race'] == 0) & (df['prediction'] == 1) & (df['two_year_recid'] == 0)].shape[0] / df[(df['race'] == 0) & (df['two_year_recid'] == 0)].shape[0]
fpr_caucasian = df[(df['race'] == 1) & (df['prediction'] == 1) & (df['two_year_recid'] == 0)].shape[0] / df[(df['race'] == 1) & (df['two_year_recid'] == 0)].shape[0]

plt.bar(['African-American', 'Caucasian'], [fpr_african_american, fpr_caucasian], color=['#c0392b', '#3498db'])
plt.title('False Positive Rates by Race (Original)')
plt.ylabel('False Positive Rate')
plt.ylim(0, 1)
plt.show()

# -------------------------------
# ⚖️ 6. Bias Mitigation: Reweighing
# -------------------------------

RW = Reweighing(unprivileged_groups=unprivileged_groups,
                privileged_groups=privileged_groups)

dataset_transf = RW.fit_transform(dataset)

# Metrics after reweighing
metric_transf = BinaryLabelDatasetMetric(dataset_transf,
                                         unprivileged_groups=unprivileged_groups,
                                         privileged_groups=privileged_groups)

print("Disparate Impact (after reweighing):", metric_transf.disparate_impact())
print("Mean difference (after reweighing):", metric_transf.mean_difference())

# -------------------------------
# 📊 7. Visualize Disparate Impact Before & After
# -------------------------------

disparate_impact_before = metric_orig.disparate_impact()
disparate_impact_after = metric_transf.disparate_impact()

plt.bar(['Original', 'Reweighed'], [disparate_impact_before, disparate_impact_after], color=['#c0392b', '#27ae60'])
plt.title('Disparate Impact Before and After Reweighing')
plt.ylabel('Disparate Impact Ratio')
plt.ylim(0, 2)
plt.axhline(0.8, color='grey', linestyle='--', label='Fairness Threshold (0.8)')
plt.legend()
plt.show()

# -------------------------------
# 📖 8. Summary
# -------------------------------

print("Summary:")
print(f"- Original Disparate Impact: {disparate_impact_before:.2f}")
print(f"- Reweighed Disparate Impact: {disparate_impact_after:.2f}")

if disparate_impact_after > 0.8:
    print("✅ Bias mitigation successful. Fairness improved after reweighing.")
else:
    print("⚠️ Fairness still below threshold. Consider additional mitigation strategies.")
